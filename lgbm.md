# LightGBMについて

## LightGBMとは?

- 決定木アルゴリズムに基づいた勾配ブースティング（Gradient Boosting）の機械学習フレームワークです。

## LightGBMの主な特徴

- Gradientbased One-Side Sampling (GOSS) GOSS:データに重み付けをする。勾配の小さいもの(既に学習ができているデータ)は以降の学習で使われにくいようにしています。

- データをhistogramの形でくくっている(histogram-based algorithm) データを一つ一つ見るのではなく、いくつかの塊としてみているイメージ。それによって、演算量を軽くしています。

- Exclusive Feature Bundling (EFB) EFB:特徴量をバンドルにまとめる役割をしています。それを通じて学習時に使用する特徴量の数を少なくさせようとしています。

## LightGBMの仕組み

LightGBMは決定木の勾配ブースティングであるため、決定木の理解が必要である。よって**決定木**、**アンサンブル学習**、**勾配ブースティング**について述べる。

### 決定木

概要：木構造のモデルによって分類する手法。~上から１つの説明変数とその閾値によってデータを２つに分け、さらに枝先で同様に別基準でデータを分けることによって、分類するモデル。~説明変数の選択と閾値は、**ジニ不純度**や**エントロピー**といった基準を用いて決定する。**非線形モデル**でありながら可読性が高い数少ないモデル。

予測対象：分類

可読性： ○

並列処理： ×

過学習防止策： 新たな基準を作成する際に必要な最小データ数の設定、新たな基準を作成する際に最適減必要な基準の改善量の設定、基準の分割後の必要データ数の設定、木の最大深さを浅くするなど

### アンサンブル学習

アンサンブル学習は複数のモデルを融合させて一つの学習モデルを生成する手法です。アンサンブル手法は大き分けて３つに分別できます。それが**スタッキング**、**バギング**、**ブースティング**です。

- バギング：それぞれのモデルを並列的に学習を行う。
- ブースティング：~前の弱学習器の結果を次の学習データに半値させる~

決定木を弱学習器として「バギング」によるアンサンブル学習の手法を::ランダムフォレスト::と呼びます。対して、勾配ブースティングは決定木を弱学習器として「ブースティング」の手法を用いてアンサンブル学習を行います。

### 勾配ブースティング

概要：決定木を大量に生成し、各決定木の結果を集計して予測する手法。決定木を逐次的に増やしていき、生成済みの決定木が間違えてしまうケースのラベルを更新して、新たな決定木を生成していくイメージ。**XGBoost**という高速なライブラリが出現し、精度が非常に高く計算時間も実用的な時間に収まるようになり、最も人気な手法である。

予測対象: 連続値(回帰木の場合)、分類

可読性： △（説明変数毎の採用量などから、重要度等の指標はわかる)

並列処理： ○

過学習防止策： 決定木の過学習防止策、決定木の本数を減らす（ブースティングを早めに止める)

## LightGBMのハイパーパラメータ

### ハイパーパラメータとは?

- 機械学習でハイパーパラメータとは機械学習手法（またはアルゴリズム）の挙動を制御する設定のこと

### 特にモデルの推測精度に影響が大きいハイパーパラメータ

- num_leaves

num_leavesは決定木の複雑度を調整します。num_leavesの値が高すぎると過学習となり、低すぎると未学習になります。num_leavesを調整する場合はmax_depth（決定木の深さ）のパラメータと一緒に調整すると良い。

- min_data_in_leaf

min_data_in_leafは決定木のノード（葉）の最小データ数を指定します。値が高いと決定木が深く育つのを抑えるため過学習防ぎますが、逆に未学習となる場合もあります。min_data_in_leafは訓練データのレコード数とnum_leavesに大きく影響される。

- max_depth

決定木の深さを指定するハイパーパラメータです。単体で調整するよりも、他のハイパーパラメータとのバランスを考えながら調整します。

1. モデル訓練のスピードを上げる
- bagging_fraction（初期値1.0）とbagging_freq（初期値0）を使う
- feature_fraction（初期値1.0）で特徴量のサブサンプリングを指定
- 小さいmax_bin（初期値 255）を使う
- save_binary（初期値 False）を使う
- 分散学習を使う

2. 推測制度を向上させる

- 大きいmax_bin（初期値255）を使う
- 小さいlearning_rate(初期値0.1)と大きいnum_iterations(初期値100)を使う
- 大きいnum_leaves（初期値31）を使う
- 訓練データのレコード数を増やす（可能ならば）

3. 過学習対策

- 小さいmax_binを使う（初期値255）
- 小さいnum_leavesを使う（初期値31）
- min_data_in_leaf（初期値20）とmin_sum_hessian_in_leaf(初期値1e-3)を使う
- bagging_fraction（初期値1.0）とbagging_freq（初期値0）を使う
- feature_fraction（初期値1.0）で特徴量のサブサンプリングを指定
- 訓練データのレコード数を増やす（可能であれば）
- lambda_l1（初期値0.0）、lambda_l2（初期値0.0）、min_gain_to_split（初期値0.0）で正則化を試す
- max_depth（初期値-1）を指定して決定木が深くならないよう調整する
- max_delta_step default = 0.0, type = double, aliases: max_tree_output, max_leaf_output